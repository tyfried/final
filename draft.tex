\documentclass{article}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{verbatim}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{subfig}
\usepackage{enumitem}

%\usepackage{caption}
%\usepackage{epstopdf}
%\usepackage{float}


\newtheorem{theorem}{Theorem}
\numberwithin{theorem}{subsection}

\theoremstyle{definition}
\newtheorem{exmp}{Example}
\numberwithin{exmp}{subsection}

\theoremstyle{definition}
\newtheorem{defn}{Definition}
\numberwithin{defn}{subsection}

\theoremstyle{definition}
\newtheorem{claim}{Claim}
\numberwithin{claim}{subsection}



\title{Exploring Bounds}
\author{Tyler Friedman}


\begin{document}
\maketitle

\section{Introduction}

\begin{comment}
PLAN
do elias, don't prove first lemma, prove second lemma and thm
do given examples, draw graphs
finish out examples of lexicodes
add graphs to plotkin
finish analysis of asymptotic singleton and plotkin
\end{comment}

An interesting problem in coding theory is determining the maximum
number of codewords in a code with certain parameters.  For an $n$-dimensional code, we are interested in 
the maximum number of codewords such that the code has minimum distance $d$.  
Formally, we use the notation $A_q(n,d)$
and $B_q(n,d)$ to define the maximum of number of codewords in a code over
$\mathbb{F}_q$ of length $n$ and minimum distance $d$ for an arbitrary (linear
on non-linear) code and linear code, respectively.  

For arbitrary $n$ and $d$, it is difficult to find $A_q(n,d)$ and $B_q(n,d)$ exactly.
 In lecture, we have considered upper bounds---Sphere Packing, Singleton, and Greismer---as well as
lower bounds---Gilbert and Varshamov---on these values.  The purpose of this
paper is to take a survey of other well-known bounds in the literature that we have not discussed.  
In the first part of the paper, we will consider the Plotkin Upper bound, 
the Elias Upper bound, and the Linear Programming Upper Bound.  In the second part of the paper, we will consider asymptotic versions
of Singleton, Plotkin, Hamming, and Elias.  (I am very certain that I won't get to discuss all of these bounds, but I am currently unsure how much space each discussion will take up so I've
listed all that I am considering.)  For each bound, we will consider its proof as well
as related examples.  Lastly, in the final part we will consider lexicodes, an interesting subset of linear codes that meet the Gilbert Bound.

Some implementation specific details: for the Linear Programming bound, I will introduce basic linear programming concepts and as well as the Krawtchouck polynomials.  For the 
Asymptotic Hamming bound I will introduce the Hilbert entropy function.  


\section{Upper Bounds}

\subsection{Plotkin Upper Bound}
%Plotkin
%talk about importance
%define it
%prove it
%work out some examples

The Plotkin Bound is an upper bound that often improves upon the Sphere Packing Bound on $A_q(n,d)$.

\begin{theorem}[Plotkin]
Let $C$ be an $(n,M,d)$ code over $\mathbb{F}_q$ such that $rn < d$ where $r = 1 - q^{-1}$.  Then
\begin{equation}
A_q(n,d) \le \left \lfloor \frac{d}{d-rn}\right\rfloor
\end{equation}
\end{theorem}

Before proving this bound, it is important to note that it is only valid when $d$ is sufficiently close to $n$.  For large $q$, this is can be a considerable limiting factor.
Consider that our restriction is $n<2d$ for $q=2$, $n<\frac{3}{2}d$ for $q=3$, $n<\frac{4}{3}d$ for for $q=4$, etc.


\begin{proof}
Let 
\begin{equation} \label{plotkin_1}
S = \sum_{x \in \mathcal{C}} \sum_{y \in \mathcal{C}} \text{d}(x,y)
\end{equation}

Note that for $x,y \in \mathcal{C}$, d$(x,y)=0$ for $x=y$, and $d \le$ d$(x,y)$ for $x\neq y$.  This implies that
\begin{equation} \label{plotkin_1_5}
M(M-1)d\le S
\end{equation}

This comes directly from equation \ref{plotkin_1}.  We have $M$ possible codewords to consider for $x$ in the outer summation, $M-1$ distinct codewords from $x$ to consider in the inner summation for $y$, 
and the distance between $x$ and $y$ will always be greater than or equal to the minimum distance $d$.

Next, let $\mathcal{M}$ be the $M \times n$ matrix whose rows are the codewords of $\mathcal{C}$.  For $1 \le i \le n$, let $n_{i,\alpha}$ be the number of times $\alpha \in
 \mathbb{F}_q$ occurs in column $i$ of $\mathcal{M}$.  Note that $\sum_{\alpha \in \mathbb{F}_q} n_{i,\alpha} = M$ for $1\le i \le n$.  This is simply because there are $M$ total
 elements of $\mathbb{F}_q$ in each column, so the sum over all $n_{i,\alpha}$ must necessarily be $M$.  

I claim that we can rewrite $S$ as a double sum over these $n_{i,\alpha}$ values.  Specifically,

\begin{equation}
S = \sum_{i=1}^{n} \sum_{\alpha\in\mathbb{F}_q} n_{i,\alpha}(M - n_{i,\alpha}).
\end{equation}

Note that $n_{i,\alpha}(M - n_{i,\alpha})$ is multiplying the number of times $\alpha$ occurs in column $i$ by the number of times it doesn't occur in column $i$.  In other words, we're counting the number of times two elements of the same
column are not equivalent, exactly the condition that constitutes our Hamming distance.   By summing over all $\alpha$ and then all columns, we arrive at the same value generated by \ref{plotkin_1}.

Next, simplify:

\begin{equation}
\begin{split}
\sum_{i=1}^{n} \sum_{\alpha\in\mathbb{F}_q} n_{i,\alpha}(M - n_{i,\alpha}) &= M\sum_{i=1}^{n} \sum_{\alpha\in\mathbb{F}_q} n_{i,\alpha} - \sum_{i=1}^{n} \sum_{\alpha\in\mathbb{F}_q} n_{i,\alpha}^2 \\
&= nM^2 - \sum_{i=1}^{n} \sum_{\alpha\in\mathbb{F}_q} n_{i,\alpha}^2
\end{split}
\end{equation}

Recall the Cauchy-Schwarz inequality, which states that $\left(\sum_{i=1}^n x_i y_i \right )^2 \le \left(\sum_{i=1}^n x_i^2 \right ) \left(\sum_{i=1}^n y_i^2 \right )$
For our purposes, this means that $\left(\sum_{\alpha\in\mathbb{F}_q} 1\cdot n_{i,\alpha} \right )^2 \le q \left(\sum_{\alpha\in\mathbb{F}_q} n_{i,\alpha}^2 \right )$.

Using this inequality, we obtain

\begin{equation} \label{plotkin_2}
S \le nM^2 - \sum_{i=1}^n q^{-1} \left(\sum_{\alpha\in\mathbb{F}_q} 1\cdot n_{i,\alpha} \right )^2 = nM^2 - \frac{nM^2}{q} = nrM^2.
\end{equation}

Combining \ref{plotkin_1_5} and \ref{plotkin_2}, we get $M(M-1)d \le nrM^2$, which simplifies to 

\begin{equation}
M \le \left\lfloor \frac{d}{d-rn} \right\rfloor.
\end{equation}

\end{proof}

It is interesting to see exactly how the Sphere Packing and Plotkin bounds compare for specific values of $n$ and $d$.  For the following examples we will assume $q=2$.    
Also, recall that if $d$ is even, $A_2(n,d) = A_2(n-1,d-1)$.  

\begin{exmp}
$(n,d) = (7,4)$.

Applying the Sphere Packing bound, we have $A_2(7,4) = A_2(6,3) \le \frac{2^6}{1 + 6}$, 
giving us $A_2(7,4) \le \textbf{9}$.

Applying the Plotkin bound, we have $A_2(7,4) \le \frac{4}{4-\frac{1}{2}\cdot7} = \textbf{8}$.

\end{exmp}

\begin{exmp}
$(n,d) = (9,6)$.

Applying Sphere Packing, we have  $A_2(9,6) = A_2(8,5) \le \frac{2^8}{1+8+28}$, giving us $A_2(n,d) \le \textbf{6}$.

Applying the Plotkin bound, we have $A_2(9,6) \le \frac{6}{6-\frac{1}{2}\cdot9} = \textbf{4}$.
\end{exmp}

It is also interesting to consider a more broader picture of what is going on here.  Below is a graph comparing rates of codes versus the Sphere Packing and Plotkin bounds
that we can calculate for those values for different values of q.  

\begin{comment}
\begin{figure}
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=.8\linewidth]{plotkin_q2.png}
  \caption{1a}
  \label{fig:sfig1}
\end{subfigure}%
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=.8\linewidth]{plotkin_q3.png}
  \caption{1b}
  \label{fig:sfig2}
\end{subfigure}
\caption{plots of....}
\label{fig:fig}
\end{figure}
\end{comment}


\section{Lexicodes}
We end our discussion on bounds with an interesting class of binary linear codes
called lexicodes, short for lexicographic codes.  Lexicodes are linear and meet the Gilbert bound; we will discuss the former and prove the latter.  Note that we are 
working in $\mathbb{F}_2$.

We construct the lexicode $\mathcal{L}$ of length $n$ and minimum distance $d$ using a basic greedy algorithm as follows.  First, order all $n$-tuples in lexicographic order:
$0\cdots000$, $0\cdots001$, $0\cdots010$, $0\cdots011$, $0\cdots100$, et cetera.  Put the zero vector $\mathbf{0}$ in $\mathcal{L}$.  Next, find the first vector $\mathbf{x}$
of weight $d$ in the lexicographic ordering, and put it in $\mathcal{L}$.  Then, find the next vector in  the lexicographic ordering whose distance from each vector in $\mathcal{L}$ is 
$d$ or more and add it to $\mathcal{L}$, repeating this process until you have used up your lexicographic list.

The set $\mathcal{L}$ is a linear code of length $n$ and minimum distance $d$.  The set $\mathcal{L}$ also contains linear subcodes which are generated by the above algorithm,
but stopping at the right spots.

\begin{theorem}
After constructing $\mathcal{L}$ as above, label the vectors $\mathbf{c}_0$, $\mathbf{c}_1,$ $\ldots$ in the order they were selected such that $\mathbf{c}_0$ is the $\mathbf{0}$ 
vector.

\begin{enumerate}
\item $\mathcal{L}$ is a linear code and the vectors $\mathbf{c}_{2^i}$ are a basis of $\mathcal{L}$.  
\item After $\mathbf{c}_{2^i}$ is chosen, the next $2^i -1$ vectors can be rewritten as $\mathbf{c}_1 + \mathbf{c}_{2^i}$, $\mathbf{c}_2 + \mathbf{c}_{2^i}$, $\ldots$, 
$\mathbf{c}_{2^i -1} + \mathbf{c}_{2^i}$.
\item Let $\mathcal{L}_i = \{\mathbf{c}_0,\mathbf{c}_1,\ldots,\mathbf{c}_{2^i -1}\}$.  Then $\mathcal{L}_i$ is an $[n,i,d]$ linear code.

\end{enumerate}

\end{theorem}

Though I won't prove this, it is not difficult to see that $1.$ and $3.$ follow from $2.$    In $2.$, we are showing that all codewords in the lexicode are simply linear combinations of 
the $c_{2^i}$ codewords, hence making the $c_{2^i}$ codewords a basis for the code.

The codes $\mathcal{L}_i$ satisfy the inclusions $\mathcal{L}_1 \subset \mathcal{L}_2 \subset \cdots \subset \mathcal{L}_k = \mathcal{L}$, where $k$ is the dimension of 
$\mathcal{L}$.  In general, this dimension value $k$ is not known until you have completed the construction as described above.

\begin{exmp}
Consider the codes $\mathcal{L}_i$ of length $5$ and minimum distance $2$.  First, we must construct the entire lexicode $\mathcal{L}$.  To do so, we enumerate
all 32 vectors of $\mathbb{F}_2^5$ in lexicographic order: $00000$,$00001$,$00010$,$\ldots$,$11111$.  We find that $\mathcal{L} = \{00000, 00011,00101,00110,01001,
01010,01100,01111,10001,10010,\\ 10100,10111,11000,11011,11101,11110\}$.  As a sanity check, note that $\left | \mathcal{L} \right | = 16$, a power of 2.  Consequently, we have
$\mathcal{L}_1 = \{00000,00011\}$, $\mathcal{L}_2 = \{00000, 00011,00101,\\ 00110\}$, $\mathcal{L}_3 = \{00000, 00011,00101,00110,01001,01010,01100,01111\}$, and
$\mathcal{L}_4 = \mathcal{L}$. We can check to make sure that each $\mathbf{c}_j$ is a linear combination of the $\mathbf{c}_{2^i}$s. Consider $\mathcal{L}_3$:  

\begin{enumerate}[start=0]
\item $\mathbf{c}_0$ is the trivial linear combination.
\item $\mathbf{c}_1 = \mathbf{c}_1$
\item $\mathbf{c}_2 = \mathbf{c}_2$
\item $\mathbf{c}_3 = \mathbf{c}_1 + \mathbf{c}_2$.  $00110 = 00011+ 00101$.
\item $\mathbf{c}_4 = 1\cdot\mathbf{c}_4$
\item $\mathbf{c}_5 = \mathbf{c}_1 + \mathbf{c}_4$. $01010 = 00011 + 01001$.
\item $\mathbf{c}_6 = \mathbf{c}_2 + \mathbf{c}_4$. $011000 = 00101 + 01001$.
\item $\mathbf{c}_7 = \mathbf{c}_3 + \mathbf{c}_4 = \mathbf{c}_1 + \mathbf{c}_2 + \mathbf{c}_4$. $01111 = 00011 + 00101+01001$.
\end{enumerate}

The same analysis applies for all $\mathcal{L}_i$s.

\end{exmp}

Note that the greedy algorithm we descibed in the beginning of the section requires a strictly lexigraphical ordering, or else the code that is produced will not necessarily
be linear.  Again consider the case of $\mathbb{F}_2^5$, but instead of the lexigraphical ordering, shift every vector up one such that the ordering is identical except now the
zero vector is at the bottom of the list.  If we run our algorithm on this ordering, we will clearly not choose the zero vector to be a part of $\mathcal{L}$ and hence 
$\mathcal{L}$ will not be linear.  

Also, note that lexicodes meet one of the bound we discussed in lecture, the Gilbert bound.  Recall the Gilbert bound:

\begin{equation}
B_q(n,d) \ge \frac{q^n}{\sum_{i=0}^{d-1} \binom{n}{i}(q-1)^{i}}
\end{equation}

\begin{claim}
Lexicodes meet the Gilbert bound.
\end{claim}

\begin{proof}
Consider an arbitrary lexicode $\mathcal{L}$.  Recall our construction of $\mathcal{L}$ with vectors in $\mathbb{F}_q^n$, wherein we visit each vector in our ordering,
and if it is distance $d$ or more to every current element in $\mathcal{L}$ then we add it to our set.  More importantly, if a vector is distance $d-1$ or less to any vector in
$\mathcal{L}$, then it is not selected.  Since we scan over every possible vector in $\mathbb{F}_q^n$, this implies that all vectors in $\mathbb{F}_q^n$ are distance less than
or equal to $d-1$ from any vector in $\mathcal{L}$, as if at any point of our algorithm we reached some $v \in \mathbb{F}_q^n$, $v \not\in \mathcal{L}$,  we would have then
added it $\mathcal{L}$, a contradiction.  Hence the covering radius of $\mathcal{L}$ is $d-1$.  

Now, it is enough to show that any code $\mathcal{C}$ with covering radius $d-1$ or less meets the Gilbert bound.  Recall that the Gilbert bound can be thought of as a restatement of the 
following fact: (number of spheres around codewords) $\cdot$ (number of vectors in each sphere) $\ge$ (number of vectors in $\mathbb{F}_q^n$).  Since our covering radius
is $d-1$ or less, none of the spheres for our lexicodes will overlap, and thus there will be no double counting of vectors when counting up the vectors in each individual sphere.
Hence, $\mathcal{C}$ meets the Gilbert bound.
\end{proof} 





\section{Asymptotic Bounds}
We now wish to consider these $A_q(n,d)$ values as n goes to infinity.  To do so, we must more formally define two terms.

In class, we have considered the \textit{rate} of a linear code, $k/n$, as one measure of the goodness of a code.  That is,
the rate tells us how much information relative to redundancy that our codewords provide.  The concept of rate can be generalized
to non-linear codes as well.  For a possibly nonlinear code over $\mathbb{F}_q$ with $M$ codewords, the \textit{rate} is defined to be $n^{-1} \log_q {M}$.  Notice that
for an $[n,k,d]$ linear code, $M = q^k$ and hence the rate is $k/n$ as we expect.

A second notion of goodness that we have discussed, but I think not formally defined, is the \textit{relative distance} of a code.  For a linear or nonlinear code of length n
has minimum distance d, this value is the ratio $d/n$. 

Consequently, for our asymptotic bounds, we are interested in the largest possible rate for a family of codes over $\mathbb{F}_q$ of lengths going to infinity with relative 
distances approaching some constant $\delta$.  In other words, we consider the equation:

\begin{equation}
\alpha_{q}(\delta) = \limsup_{n \to \infty} n^{-1} \log_q A_q(n,\delta n)
\end{equation}


[What does this imply?]

\subsection{Asymptotic Singleton Bound}

Recall the Singleton Bound from lecture:

\begin{theorem}
For $d \le n$, $A_q(n,d) \le q^{n-d+1}$.
\end{theorem}

[explain what this means, what are the implications]


\begin{theorem}
If $0 \le \delta \le 1$, then $\alpha_q(\delta) \le 1 - \delta$.
\end{theorem} 
This theorem follows directly from the Singleton bound.

\begin{proof}
\begin{equation} 
\begin{split}
\alpha_{q}(\delta) & = \limsup_{n \to \infty} n^{-1} \log_q A_q(n,\delta n) \\
& \le \limsup_{n \to \infty} n^{-1} \log_q q^{n-\delta n+1} \\
& \le \limsup_{n \to \infty} \frac{n-\delta n+1}{n} \\
& \le 1 - \delta
\end{split}
\end{equation}
\end{proof}

\subsection{Asymptotic Plotkin Bound}



%First explore linear programming bound
Asymptotic singleton, include 2 examples
Asymptotic plotkin, some examples
draw graph
Asymptotic hamming
	define hilbert, prove 135,136, define vq(n,a),
	black box 2.10.3, prove 2.10.5


Table of common$a_q(n,d)$ values
Recall basic theory: thm 2.1.2, 2.1.6

Sections


Elias
talk about importance
define it
prove two lemmas that do the heavy lifting
easy proof to finish it off
work out some examples

Linear Programming Bound
talk about importance
define it
prove it
work out some examples


Note existence of two lower bounds, say we talked about them in class


\end{document}
